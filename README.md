# ğŸš€ PPO Reinforcement Learning for LunarLander-v3

![Lunar Lander](images/lunar_lander.gif)

> An intelligent agent trained to land a spacecraft safely on the moon using **Proximal Policy Optimization (PPO)**.

![Python](https://img.shields.io/badge/Python-3.10-blue) ![Framework](https://img.shields.io/badge/Framework-Stable--Baselines3-green) ![Environment](https://img.shields.io/badge/Environment-Gymnasium-orange) ![OS](https://img.shields.io/badge/OS-Linux%20(WSL)-yellow)

## ğŸ“– é¡¹ç›®ç®€ä»‹

æ­¤é¡¹ç›®æ—¨åœ¨åˆ©ç”¨æ·±åº¦å¼ºåŒ–å­¦ä¹ ï¼ˆDeep Reinforcement Learningï¼‰è§£å†³ç»å…¸çš„ **LunarLander-v3** æ§åˆ¶é—®é¢˜ã€‚é€šè¿‡ä½¿ç”¨ **Stable-Baselines3** åº“ä¸­çš„ **PPO** ç®—æ³•ï¼Œæ™ºèƒ½ä½“ï¼ˆAgentï¼‰å­¦ä¼šäº†åœ¨ç¦»æ•£åŠ¨ä½œç©ºé—´ä¸‹æ§åˆ¶ä¸»å¼•æ“å’Œä¾§å¼•æ“ï¼Œå®ç°å…‹æœæœˆçƒé‡åŠ›å¹¶å¹³ç¨³ç€é™†ã€‚

é¡¹ç›®å®Œå…¨åœ¨ **Linux (Ubuntu on WSL)** ç¯å¢ƒä¸‹å¼€å‘ï¼Œå¹¶ä½¿ç”¨ **TensorBoard** è®°å½•è®­ç»ƒè¿‡ç¨‹ä¸­çš„å…³é”®æŒ‡æ ‡ã€‚

---

## ğŸ¥ æ•ˆæœå±•ç¤º

![Training Reward Convergence](images/ep_rew_mean.png)
![Episode Length Evolution](images/ep_len_mean.png)
![Value Network Loss](images/value_loss.png)
![Explained Variance](images/explained_variance.png)
![Entropy Loss](images/entropy_loss.png)

---

## ğŸ› ï¸ ç¯å¢ƒä¸ç®—æ³•

### ç¯å¢ƒ: LunarLander-v3
*   **ç›®æ ‡**: æ§åˆ¶ç€é™†å™¨ä»å±å¹•é¡¶éƒ¨ç§»åŠ¨åˆ°åæ ‡ (0,0) çš„ç€é™†å°ï¼ˆä¸¤é¢é»„æ——ä¹‹é—´ï¼‰ã€‚
*   **çŠ¶æ€ç©ºé—´ (8ç»´)**: åæ ‡ (x, y)ã€é€Ÿåº¦ (vx, vy)ã€è§’åº¦ã€è§’é€Ÿåº¦ã€è…¿éƒ¨è§¦åœ°ä¼ æ„Ÿå™¨çŠ¶æ€ã€‚
*   **åŠ¨ä½œç©ºé—´ (ç¦»æ•£)**:
    *   `0`: ä¸æ“ä½œ
    *   `1`: å–·å°„å·¦å¼•æ“
    *   `2`: å–·å°„ä¸»å¼•æ“
    *   `3`: å–·å°„å³å¼•æ“
*   **å¥–åŠ±æœºåˆ¶**: 
    *   å®‰å…¨ç€é™†: +100
    *   å æ¯: -100
    *   å¼•æ“å–·å°„: å¾®å°çš„è´Ÿå¥–åŠ± (æ¨¡æ‹Ÿç‡ƒæ–™æ¶ˆè€—)

### ç®—æ³•: PPO (Proximal Policy Optimization)
ç›¸æ¯”äºå…¶ä»–ç®—æ³•ï¼Œæˆ‘é€‰æ‹©äº† **PPO**ï¼ŒåŸå› å¦‚ä¸‹ï¼š
1.  **ç¨³å®šæ€§**: PPO çš„ Clip æœºåˆ¶é˜²æ­¢äº†ç­–ç•¥æ›´æ–°æ­¥å¹…è¿‡å¤§ï¼Œè®­ç»ƒæ”¶æ•›æ›´ç¨³å®šã€‚
2.  **é€‚åº”æ€§**: PPO çš„ Actor-Critic æ¶æ„å¤©ç„¶é€‚åˆæ­¤ç±»ç‰©ç†æ§åˆ¶ä»»åŠ¡ï¼ˆç±»ä¼¼æ§åˆ¶å™¨+è§‚æµ‹å™¨ï¼‰ã€‚
3.  **è¡Œä¸šæ ‡å‡†**: PPO æ˜¯ç›®å‰ OpenAI ç­‰æœºæ„çš„ä¸»æµç®—æ³•ï¼ˆUsed in ChatGPT trainingï¼‰ã€‚

---

## âš¡ å®‰è£…æŒ‡å—

æœ¬é¡¹ç›®åœ¨ **WSL (Ubuntu 22.04)** ä¸‹å¼€å‘ï¼Œä¾èµ– `swig` å’Œ `box2d`ã€‚

### 1. ç³»ç»Ÿä¾èµ–
```bash
sudo apt-get update
sudo apt-get install -y swig build-essential python3-dev
``` 
### 2.  Python ä¾èµ– 
å»ºè®®ä½¿ç”¨ Conda åˆ›å»ºè™šæ‹Ÿç¯å¢ƒï¼š
```bash
conda create -n rl_env python=3.10
conda activate rl_env
pip install "gymnasium[box2d]" stable-baselines3 tensorboard shimmy
``` 

## ğŸ“Š ç»“æœåˆ†æ

### å…³é”®æŒ‡æ ‡è§£è¯»

![Training Reward Convergence](images/ep_rew_mean.png)

#### 1.å¹³å‡å¥–åŠ± (rollout/ep_rew_mean):

*   **è¶‹åŠ¿:** æ›²çº¿ä»åˆå§‹çš„ -200ï¼ˆé¢‘ç¹å æ¯ï¼‰ä¸€è·¯ä¸Šå‡ï¼Œæœ€ç»ˆ>0ã€‚
*   **æ„ä¹‰:** è¯æ˜ Agent æˆåŠŸå­¦ä¼šäº†â€œåé‡åŠ›æ‚¬åœâ€å’Œâ€œå®šç‚¹ç€é™†â€ç­–ç•¥ã€‚

![Episode Length Evolution](images/ep_len_mean.png)

#### 2.å›åˆé•¿åº¦ (rollout/ep_len_mean):

*   **è¶‹åŠ¿:** å›åˆé•¿åº¦ä» 100 å¢åŠ åˆ° 600ã€‚
*   **æ„ä¹‰:** åˆå§‹é˜¶æ®µ Agent å¿«é€Ÿå æ¯ï¼ˆæ—¶é—´çŸ­ï¼‰ï¼›åæœŸ Agent å­¦ä¼šäº†ç©ºä¸­å§¿æ€è°ƒæ•´å’Œç¼“æ…¢ä¸‹é™ï¼ˆæ§åˆ¶è¿‡ç¨‹å˜é•¿ï¼‰ï¼Œè¿™æ˜¯å­¦ä¼šæ§åˆ¶çš„ç‰¹å¾ã€‚

![Value Network Loss](images/value_loss.png)

#### 3.ä»·å€¼æŸå¤± (train/value_loss):

*   **è¶‹åŠ¿:** è¿…é€Ÿä¸‹é™å¹¶æ”¶æ•›ã€‚
*   **æ„ä¹‰:** Critic ç½‘ç»œå¯¹å½“å‰çŠ¶æ€çš„ä»·å€¼é¢„åˆ¤è¶Šæ¥è¶Šå‡†ç¡®ï¼Œç³»ç»Ÿçš„â€œè‡ªæˆ‘è¯„ä»·ä½“ç³»â€å·²å»ºç«‹ã€‚

## ğŸ“‚ é¡¹ç›®ç»“æ„
```text
.
â”œâ”€â”€ logs/                   # TensorBoard logs
â”œâ”€â”€ models/                 # Saved PPO models (.zip)
â”œâ”€â”€ images/                 # Saved some images
â”œâ”€â”€ train.py                # Main training script
â”œâ”€â”€ README.md               # Project documentation
â””â”€â”€ .gitignore
```

Run under Linux (WSL) | Powered by Stable-Baselines3






